{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "IMl_266sSy0K",
        "outputId": "7959fac3-6c31-4330-c3f5-3dd6d224078c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "\u001b[33mWARNING: Skipping scikit-surprise as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting scikit-surprise\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise) (1.4.2)\n",
            "Collecting numpy>=1.19.5 (from scikit-surprise)\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise) (1.14.1)\n",
            "Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp311-cp311-linux_x86_64.whl size=2505215 sha256=061abbd5cc47951ca408b49d559e066c9752a78e5e2782c102833c4820d80c0c\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/8f/6e/7e2899163e2d85d8266daab4aa1cdabec7a6c56f83c015b5af\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: numpy, scikit-surprise\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5 scikit-surprise-1.1.4\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m116.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.3.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.1.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "274ff35a6402448f8148deb4798b3437"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#this step is needed to run in google colab to prevent dependency issues\n",
        "!pip uninstall numpy scikit-surprise -y\n",
        "!pip install scikit-surprise\n",
        "!pip install numpy==1.23.5\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
        "from surprise import Dataset, Reader, SVDpp\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy"
      ],
      "metadata": {
        "id": "J5tFQ6ZsTGqU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import files**\n",
        "\n",
        "Import files and run some of the data preprocessing steps written by Shreyas"
      ],
      "metadata": {
        "id": "u60kkUWzB0yL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessed files generated by code from shreyas\n",
        "#train = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "#test = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "#val = pd.read_csv('/content/drive/MyDrive/val.csv')\n",
        "train.pd.read_csv('../data/train.csv')\n",
        "test.pd.read_csv('../data/test.csv')\n",
        "val.pd.read_csv('../data/val.csv')"
      ],
      "metadata": {
        "id": "j5CYss92TJdO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code from shreyas, use the same user and restaurant features\n",
        "def create_dataset(data):\n",
        "    # User features\n",
        "    user_features = {\n",
        "        'review_count': data['review_count_norm_x'].values.astype(np.float32),\n",
        "        'average_stars': data['average_stars_norm'].values.astype(np.float32),\n",
        "        'fans': data['fans_norm'].values.astype(np.float32),\n",
        "        'friends_count': data['friends_count_norm'].values.astype(np.float32),\n",
        "        'elite': data['elite_binary'].values.astype(np.float32)\n",
        "    }\n",
        "    #added this section to convert parking column values to 1,0 due to error\n",
        "    parking_columns = ['park_garage', 'park_street', 'park_validated', 'park_lot', 'park_valet']\n",
        "    for col in parking_columns:\n",
        "        data[col] = data[col].map({'True': 1.0, 'False': 0.0, True: 1.0, False: 0.0}).fillna(0.0)\n",
        "\n",
        "    # Restaurant features\n",
        "    rest_features = {\n",
        "        'stars': data['stars_norm'].values.astype(np.float32),\n",
        "        'review_count': data['review_count_norm_y'].values.astype(np.float32),\n",
        "        'lat': data['lat_norm'].values.astype(np.float32),\n",
        "        'lon': data['lon_norm'].values.astype(np.float32),\n",
        "        'categories': data[[f'cat_{i}' for i in range(50)]].values.astype(np.float32),\n",
        "        'parking': data[['park_garage', 'park_street', 'park_validated', 'park_lot', 'park_valet']].values.astype(np.float32)\n",
        "    }\n",
        "\n",
        "    # Labels (target variable)\n",
        "    labels = data['stars'].values.astype(np.float32)\n",
        "\n",
        "    return user_features, rest_features, labels"
      ],
      "metadata": {
        "id": "FotOq271TLv4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code from shreyas - research.ipynb\n",
        "train_user, train_rest, train_labels = create_dataset(train)\n",
        "val_user, val_rest, val_labels = create_dataset(val)\n",
        "test_user, test_rest, test_labels = create_dataset(test)"
      ],
      "metadata": {
        "id": "IipC8XGTTN63"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 1: XGBoost**\n",
        "\n",
        "XGBoost with a content based approach"
      ],
      "metadata": {
        "id": "sBm6FkFQCB7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#combine features to create a model that uses restaurant features\n",
        "def combine_features(user_features, rest_features):\n",
        "    # Convert dictionaries to arrays\n",
        "    user_array = np.column_stack([user_features[k] for k in user_features.keys()])\n",
        "    rest_array = np.column_stack([rest_features[k] for k in rest_features.keys() if k != 'categories'])\n",
        "\n",
        "    # Flatten categories\n",
        "    categories_array = rest_features['categories']\n",
        "\n",
        "    # Combine all features\n",
        "    combined_features = np.column_stack([user_array, rest_array, categories_array])\n",
        "    return combined_features\n",
        "\n",
        "X_train = combine_features(train_user, train_rest)\n",
        "X_val = combine_features(val_user, val_rest)\n",
        "X_test = combine_features(test_user, test_rest)\n",
        "\n",
        "y_train = train_labels\n",
        "y_val = val_labels\n",
        "y_test = test_labels"
      ],
      "metadata": {
        "id": "pH065-6aTQXp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb\n",
        "\n",
        "# create an xgb model\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    eval_metric='rmse',\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# kept best parameters\n",
        "param_grid = {\n",
        "    'max_depth': [6],\n",
        "    'learning_rate': [0.1],\n",
        "    'n_estimators': [100],\n",
        "    'subsample': [0.8],\n",
        "    'colsample_bytree': [1.0],\n",
        "    'gamma': [0.1]\n",
        "}\n",
        "\n",
        "#use gridsearch\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_grid=param_grid,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    cv=5,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# run grid_search for the besst model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# save best model\n",
        "best_model = grid_search.best_estimator_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBD5GTHGTVBH",
        "outputId": "b50a6783-e26c-4acd-bd2f-cf107a12f9fd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evalute on validation set and test set\n",
        "xgb_val_preds = best_model.predict(X_val)\n",
        "xgb_test_preds = best_model.predict(X_test)\n",
        "\n",
        "#calculate rmse\n",
        "xgb_val_mse = mean_squared_error(y_val, xgb_val_preds)\n",
        "xgb_test_mse = mean_squared_error(y_test, xgb_test_preds)\n",
        "xgb_val_rmse = np.sqrt(xgb_val_mse)\n",
        "xgb_test_rmse = np.sqrt(xgb_test_mse)\n",
        "\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Validation RMSE: {xgb_val_rmse:.4f}\")\n",
        "print(f\"Test RMSE: {xgb_test_rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeVpGsl0Csce",
        "outputId": "2276ae31-4947-40a2-e4ea-2bf11133596c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'colsample_bytree': 1.0, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'subsample': 0.8}\n",
            "Validation RMSE: 1.0689\n",
            "Test RMSE: 1.0701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 2: SVD++**\n",
        "\n",
        "SVD++ for collaborative filtering"
      ],
      "metadata": {
        "id": "4Nli5K_0CN04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise.model_selection import GridSearchCV\n",
        "\n",
        "# Prepare the data\n",
        "train_cf = train[['user_id', 'business_id', 'stars']]\n",
        "val_cf = val[['user_id', 'business_id', 'stars']]\n",
        "test_cf = test[['user_id', 'business_id', 'stars']]\n",
        "\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "train_data = Dataset.load_from_df(train_cf, reader)\n",
        "\n",
        "# some of the better params\n",
        "param_grid = {\n",
        "    'n_factors': [10, 0],\n",
        "    'n_epochs': [20],\n",
        "    'lr_all': [0.01],\n",
        "    'reg_all': [0.3],\n",
        "    'random_state': [42]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV using SVDpp\n",
        "gs = GridSearchCV(\n",
        "    SVDpp,\n",
        "    param_grid,\n",
        "    measures=['rmse', 'mae'],\n",
        "    cv=5,\n",
        "    refit='rmse',\n",
        "    n_jobs=1,\n",
        "    joblib_verbose=2\n",
        ")\n",
        "\n",
        "# Fit the grid search\n",
        "gs.fit(train_data)\n",
        "\n",
        "# Print the results\n",
        "print(\"Best RMSE:\", gs.best_score['rmse'])\n",
        "print(\"Best MAE:\", gs.best_score['mae'])\n",
        "print(\"Best params:\", gs.best_params['rmse'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMR88MuhTYep",
        "outputId": "6c52e6ea-f5ba-4d15-fe28-d9064621fe6e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best RMSE: 1.3439488104648813\n",
            "Best MAE: 1.0996196454881395\n",
            "Best params: {'n_factors': 0, 'n_epochs': 20, 'lr_all': 0.01, 'reg_all': 0.3, 'random_state': 42}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_svd(svd_model, df):\n",
        "    testset = list(df.itertuples(index=False, name=None))\n",
        "    predictions = svd_model.test(testset)\n",
        "    return np.array([pred.est for pred in predictions])"
      ],
      "metadata": {
        "id": "2izY5HO_TbCq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svd = gs.best_estimator['rmse']\n",
        "\n",
        "# Predict using SVD\n",
        "svd_val_preds = predict_svd(svd, val_cf)\n",
        "svd_test_preds = predict_svd(svd, test_cf)\n",
        "\n",
        "# Actual ratings\n",
        "val_true = val_cf['stars'].values\n",
        "test_true = test_cf['stars'].values\n",
        "\n",
        "# Compute RMSE\n",
        "val_rmse = np.sqrt(mean_squared_error(val_true, svd_val_preds))\n",
        "test_rmse = np.sqrt(mean_squared_error(test_true, svd_test_preds))\n",
        "\n",
        "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
        "print(f\"Test RMSE: {test_rmse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3NHneCD3EvD",
        "outputId": "6a44c05a-d0ca-4ce7-b894-7610d608973a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation RMSE: 1.3332\n",
            "Test RMSE: 1.3331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1st Hybrid Approach: Random Forest**"
      ],
      "metadata": {
        "id": "KBc4cngOv7bO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_meta_features(svd_preds, xgb_preds):\n",
        "    return pd.DataFrame({\n",
        "        'svd_pred': svd_preds,\n",
        "        'xgb_pred': xgb_preds\n",
        "    })\n",
        "\n",
        "def prepare_meta_data(svd_model, xgb_model, X, df_cf):\n",
        "    svd_preds = predict_svd(svd_model, df_cf)\n",
        "    xgb_preds = xgb_model.predict(X)\n",
        "    meta_X = create_meta_features(svd_preds, xgb_preds)\n",
        "    meta_y = df_cf['stars'].values\n",
        "    return meta_X, meta_y\n",
        "\n",
        "\n",
        "# Prepare meta-features for train, test, val\n",
        "X_meta_train, y_meta_train = prepare_meta_data(svd, best_model, X_train, train_cf)\n",
        "X_meta_val, y_meta_val = prepare_meta_data(svd, best_model, X_val, val_cf)\n",
        "X_meta_test, y_meta_test = prepare_meta_data(svd, best_model, X_test, test_cf)"
      ],
      "metadata": {
        "id": "584W_YaVYhog"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Kept best hyperparameters\n",
        "param_grid = {\n",
        "    'n_estimators': [150],\n",
        "    'max_depth': [10],\n",
        "    'min_samples_split': [30],\n",
        "    'min_samples_leaf': [4],\n",
        "    'random_state': [42]\n",
        "}\n",
        "\n",
        "#call randomforest model\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "# set up gridsearch\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
        "                           cv=3, n_jobs=-1, verbose=2,\n",
        "                           scoring='neg_mean_squared_error')\n",
        "\n",
        "#fit model\n",
        "grid_search.fit(X_meta_train, y_meta_train)\n",
        "\n",
        "#save best model\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on the test set using the tuned Random Forest model\n",
        "rf_preds_val = best_rf_model.predict(X_meta_val)\n",
        "rf_preds_test = best_rf_model.predict(X_meta_test)\n",
        "rf_rmse_val = np.sqrt(mean_squared_error(y_meta_val, rf_preds_val))\n",
        "rf_rmse_test = np.sqrt(mean_squared_error(y_meta_test, rf_preds_test))\n",
        "\n",
        "# Print out the best hyperparameters and RMSE\n",
        "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
        "print(f\"Test RMSE (Random Forest): {rf_rmse_test:.4f}\")\n",
        "print(f\"Val RMSE (Random Forest): {rf_rmse_val:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W52GgYXCYrzb",
        "outputId": "198b1ad7-f7f1-4661-e29c-75f38a6b48ac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Best Hyperparameters: {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 30, 'n_estimators': 150, 'random_state': 42}\n",
            "Test RMSE (Random Forest): 1.3283\n",
            "Val RMSE (Random Forest): 1.3292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2nd Hybird Approach: Ensemble**"
      ],
      "metadata": {
        "id": "QEHS0Uryvt5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_alpha = None\n",
        "best_rmse = float(\"inf\")\n",
        "\n",
        "for alpha in np.arange(0.8, 1.00, 0.01):\n",
        "    val_blend = alpha * xgb_val_preds + (1 - alpha) * np.array(svd_val_preds)\n",
        "    rmse = np.sqrt(mean_squared_error(y_meta_val, val_blend))\n",
        "\n",
        "    if rmse < best_rmse:\n",
        "        best_rmse = rmse\n",
        "        best_alpha = alpha\n",
        "\n",
        "print(f\"Best alpha: {best_alpha:.3f}\")\n",
        "print(f\"Best Validation RMSE: {best_rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JYy79ni6m-d",
        "outputId": "3480e298-42a0-4692-9e5e-d443cb26b58b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best alpha: 0.990\n",
            "Best Validation RMSE: 1.0689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_val = val_cf['stars'].values\n",
        "y_test = test_cf['stars'].values\n",
        "\n",
        "# Blend SVD and XGBoost predictions using best alpha\n",
        "val_ensemble_preds = best_alpha * xgb_val_preds + (1 - best_alpha) * svd_val_preds\n",
        "test_ensemble_preds = best_alpha * xgb_test_preds + (1 - best_alpha) * svd_test_preds\n",
        "\n",
        "# Compute RMSE\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, val_ensemble_preds))\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, test_ensemble_preds))\n",
        "\n",
        "print(f\"Validation RMSE (Ensemble): {val_rmse:.4f}\")\n",
        "print(f\"Test RMSE (Ensemble): {test_rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crg7IzWa6rla",
        "outputId": "82b12810-e1fa-47c5-e518-137e542e6a62"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation RMSE (Ensemble): 1.0689\n",
            "Test RMSE (Ensemble): 1.0701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3rd Hybrid Approach: Residuals**"
      ],
      "metadata": {
        "id": "WT-clC9ikSYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#XGBoost predictions\n",
        "#val and test already explicitly called\n",
        "xgb_train_preds = best_model.predict(X_train)\n",
        "\n",
        "#compute residuals\n",
        "residuals_train = y_train - xgb_train_preds\n",
        "\n",
        "#build Surprise dataset with residuals\n",
        "train_resid_df = train_cf.copy()\n",
        "train_resid_df['stars'] = residuals_train\n",
        "\n",
        "data = Dataset.load_from_df(train_resid_df[['user_id', 'business_id', 'stars']], reader)\n",
        "trainset_resid = data.build_full_trainset()\n",
        "\n",
        "#train SVD++ on residuals\n",
        "svd_resid = SVDpp()\n",
        "svd_resid.fit(trainset_resid)\n",
        "\n",
        "#predict residuals on val\n",
        "val_resid_testset = list(val_cf.itertuples(index=False, name=None))\n",
        "svd_resid_preds = svd_resid.test(val_resid_testset)\n",
        "svd_resid_vals = np.array([pred.est for pred in svd_resid_preds])\n",
        "\n",
        "#predict residuals on test\n",
        "test_resid_testset = list(test_cf.itertuples(index=False, name=None))\n",
        "svd_resid_test_preds = svd_resid.test(test_resid_testset)\n",
        "svd_resid_test_vals = np.array([pred.est for pred in svd_resid_test_preds])\n",
        "\n",
        "#calculate rmse for validation\n",
        "residual_val_blend = xgb_val_preds + svd_resid_vals\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, residual_val_blend))\n",
        "print(f\"Validation RMSE (Residual Hybrid): {val_rmse:.4f}\")\n",
        "\n",
        "#calculate rmse for test\n",
        "residual_test_blend = xgb_test_preds + svd_resid_test_vals\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, residual_test_blend))\n",
        "print(f\"Test RMSE (Residual Hybrid): {test_rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIJiARgBkXBY",
        "outputId": "fd767628-dc3b-4c26-eb2b-f2bc98054f7d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation RMSE (Residual Hybrid): 1.4623\n",
            "Test RMSE (Residual Hybrid): 1.4640\n"
          ]
        }
      ]
    }
  ]
}